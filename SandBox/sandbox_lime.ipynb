{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sandbox_lime.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1bW3k4dwg1hAgBd_JoSOGaj82uS6v2lD5",
      "authorship_tag": "ABX9TyPhnZ0u2Bl+EViwkX7MREv7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hepuliu/Masters_Thesis/blob/sandbox_lime/sandbox_lime.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "4gxfMnzSnm0r"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Master Thesis Simulation Sandbox\n",
        "Flood Prevention Dam Sizing with Machine Learining Approaches - Hepu Liu"
      ],
      "metadata": {
        "id": "stxpzyENrS9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overall Project Simulation Steps\n",
        "1. Process discharge data from Waldangelbach Station\n",
        "\n",
        "2. Process precipitation data from Baiertal  Station\n",
        "\n",
        "3. Build Prediction Model (Model A)\n",
        "\n",
        "4. Process precipitation data from Stifterhof Station\n",
        "\n",
        "5. Process precipitation data from Waibstadt Station (optional)\n",
        "\n",
        "6. Process precipitation data from Stetten Station (optional\n",
        "\n",
        "7. Fit data to Model A to predict discharge\n",
        "\n",
        "### Variable Naming Conventions\n",
        "\n",
        "- Weather Stations Naming: ('p' for precipitation, 'd' for discharge, 'a' to 'd' for different stations, 'r' for result)\n",
        "\n",
        "  - da: Waldangelbach Station\n",
        "  - pa: Baiertal Station\n",
        "  - pb: Stifterhof Station\n",
        "  - pc: Waibstadt Station\n",
        "  - pd: Stetten Station\n",
        "  - pr: combined/resulting precipitation\n",
        "  - dr: predicted/resulting discharge\n",
        "\n",
        "- Variable Naming Coventions: \n",
        "  - df: data frame\n",
        "  - trs: training set\n",
        "  - tes: testing set\n",
        "  - fbp: FB Prophet\n",
        "  - lstm: LSTM\n",
        "  - cnn: CNN\n",
        "  - lstmss: LSTM-seq2sqe\n",
        "\n"
      ],
      "metadata": {
        "id": "vTni1B9BrUaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eHDXLlNlrcPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "QKzUy-aprWdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import csv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from fbprophet import Prophet\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from numpy import loadtxt\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "drive.mount"
      ],
      "metadata": {
        "id": "i3obmBLsrgIP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ed0d3e-81d4-43a1-fd7b-0503f05c14f7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function google.colab.drive.mount>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Datasets"
      ],
      "metadata": {
        "id": "OjEf5D5krZJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import datafram for CNN\n",
        "df_cnn = pd.read_csv('/content/drive/MyDrive/thesis/dataset/cleaned_df/df_cnn.csv')\n",
        "df_cnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ZnnQP7rkEatu",
        "outputId": "0e2528c5-5726-45ff-a384-703bfa4cddc3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         ds      y   temp  rad  preci\n",
              "0       2007-01-01 00:00:00  0.226  10.00  0.0    2.6\n",
              "1       2007-01-01 01:00:00  0.248  10.58  0.0    0.8\n",
              "2       2007-01-01 02:00:00  0.248  11.22  0.0    0.2\n",
              "3       2007-01-01 03:00:00  0.320  11.42  0.0    0.6\n",
              "4       2007-01-01 04:00:00  0.346  11.58  0.0    0.0\n",
              "...                     ...    ...    ...  ...    ...\n",
              "105148  2018-12-31 19:00:00  0.232   6.19  0.0    0.0\n",
              "105149  2018-12-31 20:00:00  0.248   6.23  0.0    0.1\n",
              "105150  2018-12-31 21:00:00  0.232   6.25  0.0    0.1\n",
              "105151  2018-12-31 22:00:00  0.226   6.26  0.0    0.0\n",
              "105152  2018-12-31 23:00:00  0.226   6.26  0.0    0.0\n",
              "\n",
              "[105153 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8964d595-5d66-4d44-88d3-6f21f5f5c205\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ds</th>\n",
              "      <th>y</th>\n",
              "      <th>temp</th>\n",
              "      <th>rad</th>\n",
              "      <th>preci</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2007-01-01 00:00:00</td>\n",
              "      <td>0.226</td>\n",
              "      <td>10.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2007-01-01 01:00:00</td>\n",
              "      <td>0.248</td>\n",
              "      <td>10.58</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2007-01-01 02:00:00</td>\n",
              "      <td>0.248</td>\n",
              "      <td>11.22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2007-01-01 03:00:00</td>\n",
              "      <td>0.320</td>\n",
              "      <td>11.42</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2007-01-01 04:00:00</td>\n",
              "      <td>0.346</td>\n",
              "      <td>11.58</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105148</th>\n",
              "      <td>2018-12-31 19:00:00</td>\n",
              "      <td>0.232</td>\n",
              "      <td>6.19</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105149</th>\n",
              "      <td>2018-12-31 20:00:00</td>\n",
              "      <td>0.248</td>\n",
              "      <td>6.23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105150</th>\n",
              "      <td>2018-12-31 21:00:00</td>\n",
              "      <td>0.232</td>\n",
              "      <td>6.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105151</th>\n",
              "      <td>2018-12-31 22:00:00</td>\n",
              "      <td>0.226</td>\n",
              "      <td>6.26</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105152</th>\n",
              "      <td>2018-12-31 23:00:00</td>\n",
              "      <td>0.226</td>\n",
              "      <td>6.26</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>105153 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8964d595-5d66-4d44-88d3-6f21f5f5c205')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8964d595-5d66-4d44-88d3-6f21f5f5c205 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8964d595-5d66-4d44-88d3-6f21f5f5c205');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ],
      "metadata": {
        "id": "QW4JNsmrrlZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction\n"
      ],
      "metadata": {
        "id": "wbHEawyprwZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jz6B0p3Raadn"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Archive"
      ],
      "metadata": {
        "id": "uupgN12or3e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## FBProphet - NOT USED FOR THIS THESIS\n",
        "\n",
        "# # Multi Variant Prediction Model\n",
        "# def predictor(df, reg1, reg2, reg3):\n",
        "#   predictor = Prophet(interval_width=0.5)\n",
        "#   predictor.add_regressor(reg1)\n",
        "#   predictor.add_regressor(reg2)\n",
        "#   predictor.add_regressor(reg3)\n",
        "#   predictor.fit(df)\n",
        "#   return predictor\n",
        "\n",
        "# # Make Prediction Dataframe\n",
        "# def prediction_df(predictor,df):\n",
        "#   prediction_df = predictor.predict(df).loc[:,['ds','yhat']]\n",
        "#   prediction_df['ds'] = prediction_df['ds'].apply(lambda x:x)\n",
        "#   return prediction_df\n",
        "\n",
        "\n",
        "# # Prediction for Discharge\n",
        "# predictor = predictor(df_fbp,'temp','rad','preci')\n",
        "# da_dr_fbp_predicted = prediction_df(predictor, df_fbp)\n",
        "# # da_dr_fbp = results_df(df_fbp, da_dr_fbp_predicted)\n",
        "# da_dr_fbp_predicted.to_csv('/content/drive/MyDrive/thesis/dataset/results/da_dr_fbp_predicted_2.csv', index=False)"
      ],
      "metadata": {
        "id": "VnpWNlu1vRNZ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Merge prediction df to original df\n",
        "# da_dr_fbp_predicted = pd.read_csv('/content/drive/MyDrive/thesis/dataset/results/da_dr_fbp_predicted_2.csv')\n",
        "# da_dr_fbp = da_dr_fbp_predicted.merge(df_fbp, on='ds', how='left')\n",
        "# da_dr_fbp.to_csv('/content/drive/MyDrive/thesis/dataset/results/da_dr_fbp_2.csv', index=False)"
      ],
      "metadata": {
        "id": "lmVex-n5dMkr"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Cleanup Discharge A DataFrame da_df\n",
        "# da_df = pd.read_csv('/content/drive/MyDrive/thesis/dataset/Wiesloch_waldangelbach_hourly_20070101-20210501.csv')\n",
        "# da_df = da_df.iloc[13:].reset_index(drop=True)\n",
        "# da_df.columns = da_df.iloc[0]\n",
        "# da_df = da_df.iloc[3:].reset_index(drop=True)\n",
        "# da_df = da_df.iloc[:, 4:7] # precipitation unit [m3/s]\n",
        "# da_df['Uhrzeit'] = da_df['Uhrzeit'].str.replace(' v', '')\n",
        "# da_df['t'] = pd.to_datetime(da_df['Datum']+' '+da_df['Uhrzeit'], format=('%y-%m-%d %H:%M:%S'))\n",
        "# da_df = da_df.iloc[:,2:]\n",
        "# da_df.columns = ['discharge [m3/s]', 't']\n",
        "# da_df = da_df[['t','discharge [m3/s]']]\n",
        "# da_df.to_csv('/content/drive/MyDrive/thesis/dataset/cleaned_df/da_df.csv', index=False)"
      ],
      "metadata": {
        "id": "7iutg2DHr4Ft"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Cleanup Precipitation A DataFrame pa_df\n",
        "# pa_df = pd.read_csv('/content/drive/MyDrive/thesis/dataset/Weather_station_Baiertal.csv')\n",
        "# pa_df.columns = pa_df.iloc[0]\n",
        "# pa_df = pa_df.iloc[1:].reset_index(drop=True)\n",
        "# pa_df['t'] = pd.to_datetime(pa_df['date']+' '+pa_df['time'], format=('%y-%m-%d %H:%M'))\n",
        "# pa_df = pa_df.iloc[:,2:]\n",
        "# cols = list(pa_df.columns)\n",
        "# cols = [cols[-1]] + cols[:-1]\n",
        "# pa_df = pa_df[cols]\n",
        "# pa_df.to_csv('/content/drive/MyDrive/thesis/dataset/cleaned_df/pa_df.csv', index=False)\n"
      ],
      "metadata": {
        "id": "_VfhZPcgr5ss"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Cleanup Precipitation B DataFrame pb_df\n",
        "# pb_df = pd.read_csv('/content/drive/MyDrive/thesis/dataset/Weather_station_Stifterhof.csv')\n",
        "# pb_df.columns = pb_df.iloc[0]\n",
        "# pb_df = pb_df.iloc[1:].reset_index(drop=True)\n",
        "# pb_df['t'] = pd.to_datetime(pb_df['date']+' '+pb_df['time'], format=('%y-%m-%d %H:%M'))\n",
        "# pb_df = pb_df.iloc[:,2:]\n",
        "# cols = list(pb_df.columns)\n",
        "# cols = [cols[-1]] + cols[:-1]\n",
        "# pb_df = pb_df[cols]\n",
        "# pb_df.to_csv('/content/drive/MyDrive/thesis/dataset/cleaned_df/pb_df.csv', index=False)"
      ],
      "metadata": {
        "id": "and5Zobgr7Fk"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## FBProphet - NOT USED FOR THIS THESIS\n",
        "\n",
        "# # Single Variant Prediction Model\n",
        "# def single_var_predictor(df):\n",
        "#   predictor = Prophet(interval_width=0.95)\n",
        "#   predictor.fit(df)\n",
        "#   return predictor\n",
        "\n",
        "# # Make Prediction Dataframe\n",
        "# def prediction_df(predictor,df):\n",
        "#   prediction_df = predictor.predict(df).loc[:,['ds','yhat']]\n",
        "#   prediction_df['ds'] = prediction_df['ds'].apply(lambda x:x)\n",
        "#   return prediction_df\n",
        "\n",
        "# # Prediction for Discharge [15s for 2000 rows, 45s for 20000rows with GPU, 4m for all]\n",
        "# discharge_predictor = single_var_predictor(da_df)\n",
        "# da_dr = prediction_df(discharge_predictor, da_df)\n",
        "# da_dr"
      ],
      "metadata": {
        "id": "ZQhJAEjirz4N"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot Line Graph 20000 row with GPU = 3mins\n",
        "# def line_plot(df, title):\n",
        "#   label_font = {'family':'serif', 'color':'black', 'size':'12'}\n",
        "#   title_font = {'family':'serif', 'color':'black', 'size':'14'}\n",
        "#   fig = plt.figure(figsize=(8,8))\n",
        "#   plt.plot(df['ds'], df['yhat'])\n",
        "#   plt.xlabel( 't', fontdict = label_font)\n",
        "#   plt.ylabel( 'd', fontdict = label_font)\n",
        "#   plt.title(title, fontdict = title_font)\n",
        "   \n",
        "# # line_plot(da_df, 'Discharge A')\n"
      ],
      "metadata": {
        "id": "sFFfKAUTrpSe"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Making Data Frame for FB Prophet\n",
        "\n",
        "# df_fbp = da_df\n",
        "# df_fbp = df_fbp.merge(pa_df, on = 't', how = 'left').dropna()\n",
        "# df_fbp = df_fbp.drop(columns = ['wetness of leaves \\n[%]'])\n",
        "# df_fbp.columns = ['ds','y','temp','rad','preci']\n",
        "# df_fbp\n",
        "# df_fbp.to_csv('/content/drive/MyDrive/thesis/dataset/cleaned_df/df_fbp.csv', index=False)"
      ],
      "metadata": {
        "id": "0jus93Q-xOtw"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Data Processing for Multivariable LSTM - Testing\n",
        "# # df_lstm = df_lstm.iloc[:1500, :]\n",
        "# df_lstm = df_lstm.set_index('ds')\n",
        "# df_lstm"
      ],
      "metadata": {
        "id": "Y5nMHum5aNrw"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # import dataframe for fbp\n",
        "# df_fbp = pd.read_csv('/content/drive/MyDrive/thesis/dataset/cleaned_df/df_fbp.csv')\n",
        "# df_fbp"
      ],
      "metadata": {
        "id": "0tkGrbEJ1y1U"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Making dataframe for LSTM\n",
        "# df_lstm = pd.read_csv('/content/drive/MyDrive/thesis/dataset/cleaned_df/df_fbp.csv')\n",
        "# df_lstm.to_csv('/content/drive/MyDrive/thesis/dataset/cleaned_df/df_lstm.csv', index=False)\n",
        "# df_lstm"
      ],
      "metadata": {
        "id": "Mp0sTKxyZrix"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # import precipitation dataset\n",
        "# pa_df = pd.read_csv('/content/drive/MyDrive/thesis/dataset/cleaned_df/pa_df.csv', thousands='.', decimal=',')\n",
        "# pa_df.head()"
      ],
      "metadata": {
        "id": "o7Tk8DiNweEc"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # import discharge dataset\n",
        "# da_df = pd.read_csv('/content/drive/MyDrive/thesis/dataset/cleaned_df/da_df.csv')\n",
        "# da_df.head()"
      ],
      "metadata": {
        "id": "7zillKulrhwv"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # import datafram for LSTM\n",
        "# df_lstm = pd.read_csv('/content/drive/MyDrive/thesis/dataset/cleaned_df/df_lstm.csv')\n",
        "# df_lstm"
      ],
      "metadata": {
        "id": "9jEiB2uNbpCA"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # LSTM\n",
        "# # convert series to supervised learning\n",
        "# def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "#   n_vars = 1 if type(data) is list else data.shape[1]\n",
        "#   df = DataFrame(data)\n",
        "#   cols, names = list(), list()\n",
        "# \t# input sequence (t-n, ... t-1)\n",
        "#   for i in range(n_in, 0, -1):\n",
        "#     cols.append(df.shift(i))\n",
        "#     names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "# \t# forecast sequence (t, t+1, ... t+n)\n",
        "#   for i in range(0, n_out):\n",
        "#     cols.append(df.shift(-i))\n",
        "#     if i == 0:\n",
        "#       names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "#     else:\n",
        "#       names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "# \t# put it all together\n",
        "#   agg = concat(cols, axis=1)\n",
        "#   agg.columns = names\n",
        "#   # drop rows with NaN values\n",
        "#   if dropnan:\n",
        "#     agg.dropna(inplace=True)\n",
        "#   return agg\n",
        "\n",
        "# # assigne df values\n",
        "# values = df_lstm.values\n",
        "# # ensure all data is float\n",
        "# values = values.astype('float32')\n",
        "# # normalize features\n",
        "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "# scaled = scaler.fit_transform(values)\n",
        "# # frame as supervised learning\n",
        "# df_lstm_reframed = series_to_supervised(scaled, 1, 1)\n",
        "# # drop columns not to be predicted\n",
        "# df_lstm_reframed.drop(df_lstm_reframed.columns[[5,6,7]], axis=1, inplace=True)\n",
        "# df_lstm_reframed"
      ],
      "metadata": {
        "id": "ieKBGM_wfdoR"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # LSTM\n",
        "# # split into train and test sets\n",
        "# values = df_lstm_reframed.values\n",
        "# n = 9*365*24\n",
        "# # n = 1000\n",
        "# train = values[:n, :]\n",
        "# test = values[n:, :]\n",
        "# # split into input and outputs\n",
        "# train_X, train_y = train[:, :-1], train[:, -1]\n",
        "# test_X, test_y = test[:, :-1], test[:, -1]\n",
        "# # reshape input to be 3D [samples, timesteps, features]\n",
        "# train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
        "# test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
        "# print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
      ],
      "metadata": {
        "id": "zoYG86ND3A_c"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # LSTM\n",
        "# # design network\n",
        "# model = Sequential()\n",
        "# model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "# model.add(Dense(1))\n",
        "# model.compile(loss='mae', optimizer='adam')\n",
        "# # fit network\n",
        "# history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
        "# # plot history\n",
        "# plt.plot(history.history['loss'], label='train')\n",
        "# plt.plot(history.history['val_loss'], label='test')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# fig= plt.figure(figsize=(14, 4))\n",
        "# plt.title('Insert Title')\n",
        "# plt.plot(o, label='observed', color='#00688b', linewidth=0.5)\n",
        "# plt.plot(m, label='model', color='#ee7600', linewidth=0.5)\n",
        "# plt.plot([], [], ' ', label='NSE = %.3f' % nse)\n",
        "# plt.ylabel('y label')\n",
        "# plt.ylabel('x label')\n",
        "# plt.legend()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "k1k-Fwof3U1m"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # LSTM\n",
        "# # Prediction Accuracy Evaluations\n",
        "# # make a prediction\n",
        "# yhat = model.predict(test_X)\n",
        "# test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
        "# # invert scaling for forecast\n",
        "# inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
        "# inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "# inv_yhat = inv_yhat[:,0]\n",
        "# # invert scaling for actual\n",
        "# test_y = test_y.reshape((len(test_y), 1))\n",
        "# inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
        "# inv_y = scaler.inverse_transform(inv_y)\n",
        "# inv_y = inv_y[:,0]\n",
        "\n",
        "# # calculate RMSE\n",
        "# rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "# print('Test RMSE: %.3f' % rmse)\n",
        "\n",
        "# # calculate NSE\n",
        "# nse = 1-(np.sum((inv_yhat-inv_y)**2)/np.sum((inv_y-np.mean(inv_y))**2))\n",
        "# print('Test NSE: %.3f' % nse)\n",
        "\n",
        "# # plot history\n",
        "# plt.plot(inv_y, label='train')\n",
        "# plt.plot(inv_yhat, label='predict')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "8iZcIzl_UD7-"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # LSTM\n",
        "# # Training Accuracy Evaluations\n",
        "# # make a prediction\n",
        "# yhat = model.predict(train_X)\n",
        "# train_X = train_X.reshape((train_X.shape[0], train_X.shape[2]))\n",
        "# # invert scaling for forecast\n",
        "# inv_yhat = concatenate((yhat, train_X[:, 1:]), axis=1)\n",
        "# inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "# inv_yhat = inv_yhat[:,0]\n",
        "# # invert scaling for actual\n",
        "# train_y = train_y.reshape((len(train_y), 1))\n",
        "# inv_y = concatenate((train_y, train_X[:, 1:]), axis=1)\n",
        "# inv_y = scaler.inverse_transform(inv_y)\n",
        "# inv_y = inv_y[:,0]\n",
        "\n",
        "# # calculate RMSE\n",
        "# rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "# print('Test RMSE: %.3f' % rmse)\n",
        "\n",
        "# # calculate NSE\n",
        "# nse = 1-(np.sum((inv_yhat-inv_y)**2)/np.sum((inv_y-np.mean(inv_y))**2))\n",
        "# print('Test NSE: %.3f' % nse)\n",
        "\n",
        "# # plot history\n",
        "# plt.plot(inv_y, label='train')\n",
        "# plt.plot(inv_yhat, label='predict')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "aSg71hIM4Voy"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Data Processing for MLP - NOT USED FOR THIS THESIS\n",
        "# df_mlp = df_mlp.set_index('ds')\n",
        "# df_mlp = df_mlp.iloc[:1500, :]\n",
        "# df = loadtxt(df_mlp, delimiter=',')\n",
        "# x = df_mlp.iloc[:, 1:3]\n",
        "# y = df_mlp.iloc[:, 0]\n",
        "\n",
        "# # split df into training and test set\n",
        "# x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state=10)\n",
        "\n",
        "# # print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n"
      ],
      "metadata": {
        "id": "bka0q-01EjD2"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Model for MLP - NOT USED FOR THIS THESIS\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Dense(32, input_dim=4, activation='relu'))\n",
        "# model.add(Dense(16, activation='relu'))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Dense(8,activation='relu'))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Dense(4,activation='relu'))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# model.fit(x_train,y_train, epochs = 100, batch_size=10, verbose=0)\n",
        "# # evaluation\n",
        "# loss,accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "# print('loss: %.2f, accuracy: %.2f' % ((loss*100),(accuracy*100)))\n"
      ],
      "metadata": {
        "id": "8WYg_wQWMhLA"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Making dataframe for CNN\n",
        "# df_cnn = pd.read_csv('/content/drive/MyDrive/thesis/dataset/cleaned_df/df_fbp.csv')\n",
        "# df_cnn.to_csv('/content/drive/MyDrive/thesis/dataset/cleaned_df/df_cnn.csv', index=False)\n",
        "# df_cnn"
      ],
      "metadata": {
        "id": "DzAKxnTPD9sK"
      },
      "execution_count": 72,
      "outputs": []
    }
  ]
}
